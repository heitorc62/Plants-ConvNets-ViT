PyTorch Version:  1.12.1
Torchvision Version:  0.13.1
The selected epochs is: 12
The selected device is: cuda:1
Using pretrained model!!
VGG(
  (features): Sequential(
    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
    (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (5): ReLU(inplace=True)
    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (7): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (9): ReLU(inplace=True)
    (10): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (11): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (12): ReLU(inplace=True)
    (13): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (14): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (15): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (16): ReLU(inplace=True)
    (17): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (18): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (19): ReLU(inplace=True)
    (20): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (21): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (22): ReLU(inplace=True)
    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (24): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (25): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (26): ReLU(inplace=True)
    (27): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (28): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (29): ReLU(inplace=True)
    (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (31): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (32): ReLU(inplace=True)
    (33): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (35): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (36): ReLU(inplace=True)
    (37): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (38): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (39): ReLU(inplace=True)
    (40): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (41): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (42): ReLU(inplace=True)
    (43): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))
  (classifier): Sequential(
    (0): Linear(in_features=25088, out_features=4096, bias=True)
    (1): ReLU(inplace=True)
    (2): Dropout(p=0.5, inplace=False)
    (3): Linear(in_features=4096, out_features=4096, bias=True)
    (4): ReLU(inplace=True)
    (5): Dropout(p=0.5, inplace=False)
    (6): Linear(in_features=4096, out_features=39, bias=True)
  )
)
Params to learn:
	 features.0.weight
	 features.0.bias
	 features.1.weight
	 features.1.bias
	 features.3.weight
	 features.3.bias
	 features.4.weight
	 features.4.bias
	 features.7.weight
	 features.7.bias
	 features.8.weight
	 features.8.bias
	 features.10.weight
	 features.10.bias
	 features.11.weight
	 features.11.bias
	 features.14.weight
	 features.14.bias
	 features.15.weight
	 features.15.bias
	 features.17.weight
	 features.17.bias
	 features.18.weight
	 features.18.bias
	 features.20.weight
	 features.20.bias
	 features.21.weight
	 features.21.bias
	 features.24.weight
	 features.24.bias
	 features.25.weight
	 features.25.bias
	 features.27.weight
	 features.27.bias
	 features.28.weight
	 features.28.bias
	 features.30.weight
	 features.30.bias
	 features.31.weight
	 features.31.bias
	 features.34.weight
	 features.34.bias
	 features.35.weight
	 features.35.bias
	 features.37.weight
	 features.37.bias
	 features.38.weight
	 features.38.bias
	 features.40.weight
	 features.40.bias
	 features.41.weight
	 features.41.bias
	 classifier.0.weight
	 classifier.0.bias
	 classifier.3.weight
	 classifier.3.bias
	 classifier.6.weight
	 classifier.6.bias
Epoch 0/11
----------
train Loss: 0.4564 Acc: 0.8677
val Loss: 0.0764 Acc: 0.9764
Test Accuracy: 0.058824

Epoch 1/11
----------
train Loss: 0.1667 Acc: 0.9476
val Loss: 0.0504 Acc: 0.9845
Test Accuracy: 0.050420

Epoch 2/11
----------
train Loss: 0.1244 Acc: 0.9608
val Loss: 0.0306 Acc: 0.9912
Test Accuracy: 0.050420

Epoch 3/11
----------
train Loss: 0.1085 Acc: 0.9651
val Loss: 0.0442 Acc: 0.9862
Test Accuracy: 0.042017

Epoch 4/11
----------
train Loss: 0.0928 Acc: 0.9701
val Loss: 0.0255 Acc: 0.9924
Test Accuracy: 0.042017

Epoch 5/11
----------
train Loss: 0.0841 Acc: 0.9731
val Loss: 0.0395 Acc: 0.9884
Test Accuracy: 0.042017

Epoch 6/11
----------
train Loss: 0.0762 Acc: 0.9760
val Loss: 0.0256 Acc: 0.9912
Test Accuracy: 0.058824

Epoch 7/11
----------
train Loss: 0.0667 Acc: 0.9787
val Loss: 0.0207 Acc: 0.9945
Test Accuracy: 0.050420

Epoch 8/11
----------
train Loss: 0.0650 Acc: 0.9791
val Loss: 0.0164 Acc: 0.9954
Test Accuracy: 0.058824

Epoch 9/11
----------
train Loss: 0.0620 Acc: 0.9793
val Loss: 0.0174 Acc: 0.9959
Test Accuracy: 0.042017

Epoch 10/11
----------
train Loss: 0.0571 Acc: 0.9815
val Loss: 0.0191 Acc: 0.9947
Test Accuracy: 0.058824

Epoch 11/11
----------
train Loss: 0.0558 Acc: 0.9819
val Loss: 0.0148 Acc: 0.9961
Test Accuracy: 0.058824

Training complete in 53m 56s
Best Test Acc: 0.058824
stats = 
{'train_loss': [0.45639637202663397, 0.1667480522683063, 0.12436475238009978, 0.1084916982885588, 0.09281428344319421, 0.08407001913156519, 0.07618409643395269, 0.06669378741794625, 0.06495539747748601, 0.06197956816735262, 0.05712628423811112, 0.055828511927373974], 'val_loss': [0.07636322565099629, 0.050385645857417574, 0.03062576938172098, 0.04419692340786305, 0.025450835930705174, 0.039488135598838355, 0.02562808571310002, 0.020664770598212992, 0.01635306082076548, 0.017395399159531293, 0.019079488346530737, 0.014842135999722525], 'train_acc': [0.8676901573560575, 0.947563010054556, 0.9607962487037287, 0.9651246674782451, 0.9701068578384958, 0.9730601018981919, 0.976013345957888, 0.978673519996393, 0.9790567654087199, 0.9793047477343433, 0.9814915009693854, 0.9818972902294963], 'val_acc': [0.9763751127141569, 0.9844905320108206, 0.9911632100991884, 0.9862037871956718, 0.9924256086564472, 0.9883678990081154, 0.9911632100991884, 0.9944995491433724, 0.9954012623985572, 0.9959422903516681, 0.9946798917944094, 0.9961226330027051], 'test_acc': [0.058823529411764705, 0.05042016806722689, 0.05042016806722689, 0.04201680672268907, 0.04201680672268907, 0.04201680672268907, 0.058823529411764705, 0.05042016806722689, 0.058823529411764705, 0.04201680672268907, 0.058823529411764705, 0.058823529411764705], 'test_labels': [0, 0, 0, 1, 1, 2, 2, 2, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6, 7, 7, 7, 7, 7, 7, 7, 7, 8, 8, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 11, 11, 12, 12, 12, 12, 12, 13, 13, 13, 13, 13, 14, 14, 14, 14, 14, 15, 15, 15, 15, 15, 15, 15, 16, 16, 16, 16, 17, 17, 17, 17, 18, 18, 18, 18], 'test_preds': [30, 31, 4, 27, 4, 1, 1, 32, 26, 4, 4, 4, 4, 4, 8, 4, 8, 8, 8, 4, 4, 10, 10, 14, 8, 10, 9, 4, 4, 4, 4, 4, 10, 9, 9, 11, 9, 8, 8, 8, 10, 4, 8, 10, 4, 4, 4, 4, 1, 4, 4, 31, 9, 31, 12, 1, 14, 14, 4, 4, 4, 4, 4, 4, 4, 5, 4, 16, 4, 31, 4, 4, 17, 21, 17, 17, 27, 22, 12, 17, 4, 17, 30, 19, 30, 21, 12, 4, 21, 31, 27, 9, 9, 27, 27, 30, 21, 30, 4, 9, 30, 30, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 30, 33, 31, 33, 33, 30, 33]}
