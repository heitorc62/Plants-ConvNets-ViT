{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Introdução**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O objetivo desse trabalho é implementar \"Automated Bias Discover Task\" descrito em: _Discover the Unknown Biased Attribute of an Image Classifier\n",
    "Zhiheng Li, Chenliang Xu_\n",
    "\n",
    "O workflow descrito no paper, pode ser bem resumido pela seguinte imagem:\n",
    "\n",
    "# ![title](images/workflow.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Percaba que primeiramente precisamos obter um hiperplano do atributo enviesado e, depois disso, a tarefa de encontrar o viés do classificador continua.\n",
    "Nesse sentido, o escopo desse trabalho é, primeiramente obter esse hiperplano otimizado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para fazer isso, precisamos de duas coisas:\n",
    "1. Um classificador enviesado ✅\n",
    "2. Um modelo gerativo capaz de reproduzir a distribuição de probabilidades do dataset desse classificador ✅"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como já possuímos esses dois pre-requisitos, podemos implementar esse workflow. É importante perceber que faremos uma abordagem 1 vs all para cada classe no problema. Podendo encontrar diversos vieses presentes nesse dataset, um para cada classe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Implementação**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para implementar esse precisamos saber a dimensão $d$ do espaço latente do modelo gerativo treinado. Essa será a dimensionalidade do \"hiperplano do atributo enviesado\" que será encontrado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch import linalg as LA\n",
    "from torchvision import models\n",
    "from StyleGAN.modules.stylegan import Generator\n",
    "from torch import optim\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Hyperplane():\n",
    "    def __init__(self, weights, bias):\n",
    "        self.w = weights\n",
    "        self.b = bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiasDiscoverer(nn.Module):\n",
    "    def __init__(self, z_dim, generative_model, classifier, num_latent_codes=6, starting_alpha=-3, terminating_alpha=3):\n",
    "        super(BiasDiscoverer, self).__init__()\n",
    "        w = nn.Parameter(torch.randn(1, z_dim))\n",
    "        b = nn.Parameter(torch.randn(1, 1))\n",
    "        self.hyperplane = Hyperplane(w, b)\n",
    "        self.generative_model = generative_model\n",
    "        self.biased_classifier = classifier\n",
    "        self.alphas = self.get_alphas(num_latent_codes, starting_alpha, terminating_alpha) \n",
    "    \n",
    "    def get_alphas(self, num_alphas, starting_alpha, terminating_alpha):\n",
    "        step = (terminating_alpha - starting_alpha)/num_alphas\n",
    "        alphas = torch.arange(starting_alpha, terminating_alpha, step).unsqueeze(1).unsqueeze(2)\n",
    "        return alphas\n",
    "\n",
    "    def generate_latent_codes(self, z_points):\n",
    "        w = self.hyperplane.w\n",
    "        b = self.hyperplane.b\n",
    "        z_proj = z_points - ( ( (w.T @ z_points) + b ) / ( LA.vector_norm(w)**2 ) ) @ w\n",
    "        latent_codes = z_proj + ( self.alphas * ( w / LA.vector_norm(w) ) )\n",
    "        return latent_codes\n",
    "\n",
    "    def forward(self, z_points):\n",
    "        latent_codes = self.generate_latent_codes(z_points)\n",
    "        traversal_images = self.generative_model(latent_codes)\n",
    "        probs = self.biased_classifier(traversal_images)\n",
    "        return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TotalVARLoss(probs):\n",
    "    return torch.log(1e-10 + torch.abs(probs[:, 1:] - probs[:, :-1]).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_hyperplane(bias_discoverer, optimizer, EPOCHS, BATCH_SIZE, Z_DIM, DEVICE):\n",
    "    losses = []\n",
    "    print(\"Starting Training Loop...\")\n",
    "    for epoch in range(EPOCHS):\n",
    "        z_data_points = torch.rand(BATCH_SIZE, Z_DIM)\n",
    "        probs_predictions = bias_discoverer.forward(z_data_points)\n",
    "        loss = TotalVARLoss(probs_predictions)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.append(loss)\n",
    "\n",
    "    return losses\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_gen_model(PATH, DEVICE, LOG_RESOLUTION=8, W_DIM=256):\n",
    "    gen = Generator(LOG_RESOLUTION, W_DIM)\n",
    "    gen.load_state_dict(torch.load(PATH))\n",
    "    gen.to(DEVICE)\n",
    "    gen.eval()\n",
    "    return gen\n",
    "\n",
    "\n",
    "def load_classifier(PATH, DEVICE):\n",
    "    classifier = models.vgg16_bn()\n",
    "    classifier.load_state_dict(torch.load(PATH))\n",
    "    classifier.to(DEVICE)\n",
    "    classifier.eval()\n",
    "    return classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_path(path):\n",
    "    dir = os.path.dirname(path)\n",
    "    if dir: \n",
    "        if not os.path.exists(dir):\n",
    "            os.makedirs(dir)\n",
    "\n",
    "def save_statistics(losses, current_dir):\n",
    "    losses_np = np.array([loss for loss in losses])\n",
    "    losses_path = os.path.join(current_dir, \"statistics/losses.csv\")\n",
    "    make_path(losses_path)\n",
    "    np.savetxt(losses_path, losses_np, delimiter=\",\")\n",
    "\n",
    "def save_graphics(losses, current_dir):\n",
    "    plt.figure(figsize=(10,5))\n",
    "    plt.title(\"Total Variation Loss During Training\")\n",
    "    plt.plot(losses)\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plot_path = os.path.join(current_dir, \"statistics/training_loss_plot.png\")\n",
    "    make_path(plot_path)\n",
    "    plt.savefig(plot_path)\n",
    "    plt.close()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preview_saving(model):\n",
    "    # Print model's state_dict\n",
    "    print(\"Model's state_dict:\")\n",
    "    for param_tensor in model.state_dict():\n",
    "        print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(args):\n",
    "    Z_DIM = args.Z_DIM\n",
    "    LEARNING_RATE = args.LEARNING_RATE\n",
    "    EPOCHS = args.EPOCHS\n",
    "    BATCH_SIZE = args.BATCH_SIZE\n",
    "    DEVICE = args.DEVICE\n",
    "    GENERATOR_PATH = args.GENERATOR_PATH\n",
    "    CLASSIFIER_PATH = args.CLASSIFIER_PATH\n",
    "\n",
    "    gen_model = load_gen_model(GENERATOR_PATH, DEVICE)\n",
    "\n",
    "    biased_classifier = load_classifier(CLASSIFIER_PATH, DEVICE)\n",
    "\n",
    "    bias_discoverer = BiasDiscoverer(Z_DIM, gen_model, biased_classifier)\n",
    "\n",
    "    preview_saving(bias_discoverer)\n",
    "\n",
    "    optimizer = optim.Adam(bias_discoverer.parameters(), lr=LEARNING_RATE)\n",
    "    \n",
    "    # losses = optimize_hyperplane(bias_discoverer, optimizer, EPOCHS, BATCH_SIZE, Z_DIM, DEVICE)\n",
    "\n",
    "    # save_statistics(losses)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IC",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
