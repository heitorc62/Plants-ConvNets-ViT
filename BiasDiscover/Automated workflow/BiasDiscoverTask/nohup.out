Starting Training Loop...
Epoch:  0 Traceback (most recent call last):
  File "bias_discover_training_task.py", line 70, in <module>
    main(args)
  File "bias_discover_training_task.py", line 42, in main
    losses, biased_discoverer = optimize_hyperplane(bias_discoverer, biased_classifier, gen_model, optimizer, EPOCHS, BATCH_SIZE, LOG_RESOLUTION, W_DIM, TARGET_CLASS ,DEVICE)
  File "/home/heitorc62/PlantsConv/Plants-ConvNets-ViT/BiasDiscover/Automated workflow/BiasDiscoverTask/modules/train.py", line 32, in optimize_hyperplane
    loss.backward()
  File "/home/heitorc62/miniconda3/envs/heitor_env/lib/python3.7/site-packages/torch/_tensor.py", line 396, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/heitorc62/miniconda3/envs/heitor_env/lib/python3.7/site-packages/torch/autograd/__init__.py", line 175, in backward
    allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn
