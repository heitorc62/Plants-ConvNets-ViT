{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%matplotlib inline\n",
    "from modules.dcgan import Generator, Discriminator, weights_init\n",
    "from modules.train import train_model\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from matplotlib.animation import PillowWriter\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "manualSeed = 999\n",
    "random.seed(manualSeed)\n",
    "torch.manual_seed(manualSeed)\n",
    "torch.use_deterministic_algorithms(True) # Needed for reproducible results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#current_dir = os.path.dirname(os.path.realpath(__file__))                        # Get the directory of the current file (python scripts)\n",
    "current_dir = os.getcwd()                                                         # Get the directory of the current file (jupyter notebooks)     \n",
    "data_dir = \"../../../dataset/Plant_leave_diseases_dataset_without_augmentation\"   # Root directory for dataset\n",
    "workers = 4            # Number of workers for dataloader\n",
    "batch_size = 128       # Batch size during training\n",
    "image_size = 256       # Spatial size of training images\n",
    "nc = 3                 # Number of channels in the training images\n",
    "nz = 100               # Size of z latent vector (i.e. size of generator input)\n",
    "ngf = 64               # Size of feature maps in generator\n",
    "ndf = 64               # Size of feature maps in discriminator\n",
    "num_epochs = 5         # Number of training epochs\n",
    "lr = 0.0002            # Learning rate for optimizers\n",
    "beta1 = 0.5            # Beta1 hyperparameter for Adam optimizers\n",
    "ngpu = 0               # Number of GPUs available. Use 0 for CPU mode.\n",
    "real_label = 1.\n",
    "fake_label = 0.\n",
    "device = torch.device(\"cuda:0\" if (torch.cuda.is_available() and ngpu > 0) else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize(image_size),\n",
    "    transforms.CenterCrop(image_size),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "# Load the dataset\n",
    "dataset = datasets.ImageFolder(data_dir, transform=transform)\n",
    "\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "netG = Generator(ngpu, nz, ngf, nc).to(device)\n",
    "netD = Discriminator(ngpu, nc, ndf).to(device)\n",
    "\n",
    "# Handle multi-GPU if desired\n",
    "if (device.type == 'cuda') and (ngpu > 1):\n",
    "    netG = nn.DataParallel(netG, list(range(ngpu)))\n",
    "    netD = nn.DataParallel(netD, list(range(ngpu)))\n",
    "\n",
    "# Apply the ``weights_init`` function to randomly initialize all weights\n",
    "#  to ``mean=0``, ``stdev=0.02``.\n",
    "_ = netG.apply(weights_init)\n",
    "_ = netD.apply(weights_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the BCELoss function\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# Create batch of latent vectors to visualize the progression of the generator\n",
    "fixed_noise = torch.randn(batch_size//2, nz, 1, 1, device=device)\n",
    "\n",
    "optimizerD = optim.Adam(netD.parameters(), lr=lr, betas=(beta1, 0.999))\n",
    "optimizerG = optim.Adam(netG.parameters(), lr=lr, betas=(beta1, 0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training Loop...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/heitor/USP/IC/FAPESP/code_dataset/Plants-ConvNets-ViT/BiasDiscover/Automated workflow/dcgan3.ipynb Cell 6\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/heitor/USP/IC/FAPESP/code_dataset/Plants-ConvNets-ViT/BiasDiscover/Automated%20workflow/dcgan3.ipynb#W6sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m netG, netD, G_losses, D_losses, img_list \u001b[39m=\u001b[39m train_model(netG, netD, criterion, real_label, fake_label, optimizerD, optimizerG, dataloader, fixed_noise, device, nz, num_epochs)\n",
      "File \u001b[0;32m~/USP/IC/FAPESP/code_dataset/Plants-ConvNets-ViT/BiasDiscover/Automated workflow/modules/train.py:34\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(netG, netD, criterion, real_label, fake_label, optimizerD, optimizerG, dataloader, fixed_noise, device, nz, num_epochs)\u001b[0m\n\u001b[1;32m     32\u001b[0m errD_real \u001b[39m=\u001b[39m criterion(output, label)\n\u001b[1;32m     33\u001b[0m \u001b[39m# Calculate gradients for D in backward pass\u001b[39;00m\n\u001b[0;32m---> 34\u001b[0m errD_real\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     35\u001b[0m D_x \u001b[39m=\u001b[39m output\u001b[39m.\u001b[39mmean()\u001b[39m.\u001b[39mitem()\n\u001b[1;32m     37\u001b[0m \u001b[39m## Train with all-fake batch\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[39m# Generate batch of latent vectors\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/IC/lib/python3.11/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    489\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/IC/lib/python3.11/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "netG, netD, G_losses, D_losses, img_list = train_model(netG, netD, criterion, real_label, fake_label, optimizerD, optimizerG, dataloader, fixed_noise, device, nz, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the Generator\n",
    "netG_path = os.path.join(current_dir, \"models/netG.pth\")\n",
    "torch.save(netG.state_dict(), netG_path)\n",
    "\n",
    "# Save the Discriminator\n",
    "netD_path = os.path.join(current_dir, \"models/netD.pth\")\n",
    "torch.save(netD.state_dict(), netD_path)\n",
    "\n",
    "# Save the Generator loss history\n",
    "G_losses_np = np.array([loss.item() for loss in G_losses])\n",
    "G_losses_path = os.path.join(current_dir, \"statistics/G_losses.csv\")\n",
    "np.savetxt(G_losses_path, G_losses_np, delimiter=\",\")\n",
    "\n",
    "#Save the Discriminator loss history\n",
    "D_losses_np = np.array([loss.item() for loss in D_losses])\n",
    "D_losses_path = os.path.join(current_dir, \"statistics/D_losses.csv\")\n",
    "np.savetxt(D_losses_path, D_losses_np, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "plt.title(\"Generator and Discriminator Loss During Training\")\n",
    "plt.plot(G_losses,label=\"G\")\n",
    "plt.plot(D_losses,label=\"D\")\n",
    "plt.xlabel(\"iterations\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plot_path = os.path.join(current_dir, \"statistics/training_loss_plot.png\")\n",
    "plt.savefig(plot_path)\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8,8))\n",
    "plt.axis(\"off\")\n",
    "ims = [[plt.imshow(np.transpose(i,(1,2,0)), animated=True)] for i in img_list]\n",
    "ani = animation.ArtistAnimation(fig, ims, interval=1000, repeat_delay=1000, blit=True)\n",
    "\n",
    "# Save the animation\n",
    "ani.save(\"GAN_results.gif\", writer=PillowWriter(fps=1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IC",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
