PyTorch Version:  1.12.1
Torchvision Version:  0.13.1
The selected device is: cuda:1
VGG(
  (features): Sequential(
    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
    (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (5): ReLU(inplace=True)
    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (7): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (9): ReLU(inplace=True)
    (10): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (11): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (12): ReLU(inplace=True)
    (13): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (14): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (15): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (16): ReLU(inplace=True)
    (17): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (18): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (19): ReLU(inplace=True)
    (20): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (21): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (22): ReLU(inplace=True)
    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (24): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (25): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (26): ReLU(inplace=True)
    (27): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (28): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (29): ReLU(inplace=True)
    (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (31): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (32): ReLU(inplace=True)
    (33): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (35): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (36): ReLU(inplace=True)
    (37): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (38): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (39): ReLU(inplace=True)
    (40): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (41): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (42): ReLU(inplace=True)
    (43): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))
  (classifier): Sequential(
    (0): Linear(in_features=25088, out_features=4096, bias=True)
    (1): ReLU(inplace=True)
    (2): Dropout(p=0.5, inplace=False)
    (3): Linear(in_features=4096, out_features=4096, bias=True)
    (4): ReLU(inplace=True)
    (5): Dropout(p=0.5, inplace=False)
    (6): Linear(in_features=4096, out_features=39, bias=True)
  )
)
Params to learn:
	 classifier.6.weight
	 classifier.6.bias
Epoch 0/29
----------
last_layer ---> train Loss: 1.0696 Acc: 0.6998
last_layer ---> val Loss: 0.5320 Acc: 0.8425

Epoch 1/29
----------
last_layer ---> train Loss: 0.8149 Acc: 0.7576
last_layer ---> val Loss: 0.4795 Acc: 0.8564

Epoch 2/29
----------
last_layer ---> train Loss: 0.7662 Acc: 0.7676
last_layer ---> val Loss: 0.4333 Acc: 0.8712

Epoch 3/29
----------
last_layer ---> train Loss: 0.7521 Acc: 0.7713
last_layer ---> val Loss: 0.4312 Acc: 0.8720

Epoch 4/29
----------
last_layer ---> train Loss: 0.7530 Acc: 0.7732
last_layer ---> val Loss: 0.4213 Acc: 0.8730

Epoch 5/29
----------
last_layer ---> train Loss: 0.7397 Acc: 0.7776
last_layer ---> val Loss: 0.4090 Acc: 0.8768

Epoch 6/29
----------
last_layer ---> train Loss: 0.7320 Acc: 0.7791
last_layer ---> val Loss: 0.3858 Acc: 0.8815

Epoch 7/29
----------
last_layer ---> train Loss: 0.7354 Acc: 0.7786
last_layer ---> val Loss: 0.3871 Acc: 0.8808

Epoch 8/29
----------
last_layer ---> train Loss: 0.7292 Acc: 0.7807
last_layer ---> val Loss: 0.3825 Acc: 0.8822

Epoch 9/29
----------
last_layer ---> train Loss: 0.7292 Acc: 0.7788
last_layer ---> val Loss: 0.3950 Acc: 0.8785

Epoch 10/29
----------
last_layer ---> train Loss: 0.7206 Acc: 0.7822
last_layer ---> val Loss: 0.3739 Acc: 0.8868

Epoch 11/29
----------
last_layer ---> train Loss: 0.7229 Acc: 0.7843
last_layer ---> val Loss: 0.3632 Acc: 0.8889

Epoch 12/29
----------
last_layer ---> train Loss: 0.7156 Acc: 0.7857
last_layer ---> val Loss: 0.3604 Acc: 0.8869

Epoch 13/29
----------
last_layer ---> train Loss: 0.7171 Acc: 0.7851
last_layer ---> val Loss: 0.3923 Acc: 0.8795

Epoch 14/29
----------
last_layer ---> train Loss: 0.7182 Acc: 0.7857
last_layer ---> val Loss: 0.3745 Acc: 0.8841

Epoch 15/29
----------
last_layer ---> train Loss: 0.7212 Acc: 0.7845
last_layer ---> val Loss: 0.3523 Acc: 0.8928

Epoch 16/29
----------
last_layer ---> train Loss: 0.7188 Acc: 0.7843
last_layer ---> val Loss: 0.4214 Acc: 0.8836

Epoch 17/29
----------
last_layer ---> train Loss: 0.7137 Acc: 0.7856
last_layer ---> val Loss: 0.3768 Acc: 0.8854

Epoch 18/29
----------
last_layer ---> train Loss: 0.7144 Acc: 0.7865
last_layer ---> val Loss: 0.3713 Acc: 0.8876

Epoch 19/29
----------
last_layer ---> train Loss: 0.7080 Acc: 0.7889
last_layer ---> val Loss: 0.3805 Acc: 0.8889

Epoch 20/29
----------
last_layer ---> train Loss: 0.7200 Acc: 0.7863
last_layer ---> val Loss: 0.3276 Acc: 0.9012

Epoch 21/29
----------
last_layer ---> train Loss: 0.7080 Acc: 0.7877
last_layer ---> val Loss: 0.3378 Acc: 0.8965

Epoch 22/29
----------
last_layer ---> train Loss: 0.7077 Acc: 0.7877
last_layer ---> val Loss: 0.3926 Acc: 0.8886

Epoch 23/29
----------
last_layer ---> train Loss: 0.7160 Acc: 0.7852
last_layer ---> val Loss: 0.3457 Acc: 0.8950

Epoch 24/29
----------
last_layer ---> train Loss: 0.7244 Acc: 0.7859
last_layer ---> val Loss: 0.3959 Acc: 0.8866

Epoch 25/29
----------
last_layer ---> train Loss: 0.7237 Acc: 0.7842
last_layer ---> val Loss: 0.3419 Acc: 0.8940

Epoch 26/29
----------
last_layer ---> train Loss: 0.7152 Acc: 0.7855
last_layer ---> val Loss: 0.3941 Acc: 0.8860

Epoch 27/29
----------
last_layer ---> train Loss: 0.6995 Acc: 0.7893
last_layer ---> val Loss: 0.3718 Acc: 0.8823

Epoch 28/29
----------
last_layer ---> train Loss: 0.7083 Acc: 0.7871
last_layer ---> val Loss: 0.3695 Acc: 0.8925

Epoch 29/29
----------
last_layer ---> train Loss: 0.7073 Acc: 0.7867
last_layer ---> val Loss: 0.3329 Acc: 0.8948

last_layer ---> Training complete in 160m 38s
last_layer ---> Best val Acc: 0.901172
############################################## last_layer ###############################################################
