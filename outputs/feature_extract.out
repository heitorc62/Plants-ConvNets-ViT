PyTorch Version:  1.12.1
Torchvision Version:  0.13.1
The selected device is: cuda:1
VGG(
  (features): Sequential(
    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
    (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (5): ReLU(inplace=True)
    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (7): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (9): ReLU(inplace=True)
    (10): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (11): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (12): ReLU(inplace=True)
    (13): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (14): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (15): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (16): ReLU(inplace=True)
    (17): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (18): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (19): ReLU(inplace=True)
    (20): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (21): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (22): ReLU(inplace=True)
    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (24): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (25): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (26): ReLU(inplace=True)
    (27): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (28): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (29): ReLU(inplace=True)
    (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (31): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (32): ReLU(inplace=True)
    (33): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (35): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (36): ReLU(inplace=True)
    (37): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (38): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (39): ReLU(inplace=True)
    (40): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (41): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (42): ReLU(inplace=True)
    (43): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))
  (classifier): Sequential(
    (0): Linear(in_features=25088, out_features=4096, bias=True)
    (1): ReLU(inplace=True)
    (2): Dropout(p=0.5, inplace=False)
    (3): Linear(in_features=4096, out_features=4096, bias=True)
    (4): ReLU(inplace=True)
    (5): Dropout(p=0.5, inplace=False)
    (6): Linear(in_features=4096, out_features=39, bias=True)
  )
)
Params to learn:
	 classifier.6.weight
	 classifier.6.bias
Epoch 0/29
----------
last_layer ---> train Loss: 1.0663 Acc: 0.7006
last_layer ---> val Loss: 0.5231 Acc: 0.8434

Epoch 1/29
----------
last_layer ---> train Loss: 0.8182 Acc: 0.7570
last_layer ---> val Loss: 0.4824 Acc: 0.8553

Epoch 2/29
----------
last_layer ---> train Loss: 0.7741 Acc: 0.7686
last_layer ---> val Loss: 0.4447 Acc: 0.8654

Epoch 3/29
----------
last_layer ---> train Loss: 0.7540 Acc: 0.7717
last_layer ---> val Loss: 0.4252 Acc: 0.8725

Epoch 4/29
----------
last_layer ---> train Loss: 0.7475 Acc: 0.7733
last_layer ---> val Loss: 0.4235 Acc: 0.8748

Epoch 5/29
----------
last_layer ---> train Loss: 0.7369 Acc: 0.7757
last_layer ---> val Loss: 0.4344 Acc: 0.8689

Epoch 6/29
----------
last_layer ---> train Loss: 0.7351 Acc: 0.7775
last_layer ---> val Loss: 0.3891 Acc: 0.8777

Epoch 7/29
----------
last_layer ---> train Loss: 0.7192 Acc: 0.7835
last_layer ---> val Loss: 0.4151 Acc: 0.8718

Epoch 8/29
----------
last_layer ---> train Loss: 0.7203 Acc: 0.7802
last_layer ---> val Loss: 0.3771 Acc: 0.8870

Epoch 9/29
----------
last_layer ---> train Loss: 0.7217 Acc: 0.7846
last_layer ---> val Loss: 0.3945 Acc: 0.8787

Epoch 10/29
----------
last_layer ---> train Loss: 0.7201 Acc: 0.7814
last_layer ---> val Loss: 0.3566 Acc: 0.8895

Epoch 11/29
----------
last_layer ---> train Loss: 0.7134 Acc: 0.7864
last_layer ---> val Loss: 0.3964 Acc: 0.8772

Epoch 12/29
----------
last_layer ---> train Loss: 0.7310 Acc: 0.7810
last_layer ---> val Loss: 0.3803 Acc: 0.8876

Epoch 13/29
----------
last_layer ---> train Loss: 0.7096 Acc: 0.7853
last_layer ---> val Loss: 0.3991 Acc: 0.8832

Epoch 14/29
----------
last_layer ---> train Loss: 0.7103 Acc: 0.7867
last_layer ---> val Loss: 0.4144 Acc: 0.8823

Epoch 15/29
----------
last_layer ---> train Loss: 0.7131 Acc: 0.7884
last_layer ---> val Loss: 0.3633 Acc: 0.8922

Epoch 16/29
----------
last_layer ---> train Loss: 0.7220 Acc: 0.7826
last_layer ---> val Loss: 0.3777 Acc: 0.8848

Epoch 17/29
----------
last_layer ---> train Loss: 0.7122 Acc: 0.7843
last_layer ---> val Loss: 0.4202 Acc: 0.8770

Epoch 18/29
----------
last_layer ---> train Loss: 0.6987 Acc: 0.7891
last_layer ---> val Loss: 0.3485 Acc: 0.8873

Epoch 19/29
----------
last_layer ---> train Loss: 0.7080 Acc: 0.7884
last_layer ---> val Loss: 0.4023 Acc: 0.8784

Epoch 20/29
----------
last_layer ---> train Loss: 0.7227 Acc: 0.7859
last_layer ---> val Loss: 0.3718 Acc: 0.8860

Epoch 21/29
----------
last_layer ---> train Loss: 0.7113 Acc: 0.7884
last_layer ---> val Loss: 0.4344 Acc: 0.8756

Epoch 22/29
----------
last_layer ---> train Loss: 0.7069 Acc: 0.7898
last_layer ---> val Loss: 0.3710 Acc: 0.8889

Epoch 23/29
----------
last_layer ---> train Loss: 0.7105 Acc: 0.7870
last_layer ---> val Loss: 0.4140 Acc: 0.8830

Epoch 24/29
----------
last_layer ---> train Loss: 0.7154 Acc: 0.7863
last_layer ---> val Loss: 0.3414 Acc: 0.8926

Epoch 25/29
----------
last_layer ---> train Loss: 0.7079 Acc: 0.7914
last_layer ---> val Loss: 0.4654 Acc: 0.8724

Epoch 26/29
----------
last_layer ---> train Loss: 0.7097 Acc: 0.7883
last_layer ---> val Loss: 0.3771 Acc: 0.8912

Epoch 27/29
----------
last_layer ---> train Loss: 0.7141 Acc: 0.7859
last_layer ---> val Loss: 0.3528 Acc: 0.8880

Epoch 28/29
----------
last_layer ---> train Loss: 0.7127 Acc: 0.7875
last_layer ---> val Loss: 0.3463 Acc: 0.8878

Epoch 29/29
----------
last_layer ---> train Loss: 0.7043 Acc: 0.7907
last_layer ---> val Loss: 0.3539 Acc: 0.8885

last_layer ---> Training complete in 160m 47s
last_layer ---> Best val Acc: 0.892606
############################################## last_layer ###############################################################
