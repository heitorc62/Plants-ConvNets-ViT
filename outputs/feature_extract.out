PyTorch Version:  1.12.1
Torchvision Version:  0.13.1
The selected epochs is: 30
The selected feature_extract is: True
The selected use_pretrained is: True
The selected mode is: last_layer
The selected device is: cuda:1
VGG(
  (features): Sequential(
    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
    (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (5): ReLU(inplace=True)
    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (7): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (9): ReLU(inplace=True)
    (10): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (11): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (12): ReLU(inplace=True)
    (13): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (14): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (15): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (16): ReLU(inplace=True)
    (17): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (18): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (19): ReLU(inplace=True)
    (20): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (21): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (22): ReLU(inplace=True)
    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (24): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (25): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (26): ReLU(inplace=True)
    (27): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (28): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (29): ReLU(inplace=True)
    (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (31): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (32): ReLU(inplace=True)
    (33): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (35): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (36): ReLU(inplace=True)
    (37): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (38): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (39): ReLU(inplace=True)
    (40): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (41): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (42): ReLU(inplace=True)
    (43): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))
  (classifier): Sequential(
    (0): Linear(in_features=25088, out_features=4096, bias=True)
    (1): ReLU(inplace=True)
    (2): Dropout(p=0.5, inplace=False)
    (3): Linear(in_features=4096, out_features=4096, bias=True)
    (4): ReLU(inplace=True)
    (5): Dropout(p=0.5, inplace=False)
    (6): Linear(in_features=4096, out_features=39, bias=True)
  )
)
Params to learn:
	 classifier.6.weight
	 classifier.6.bias
Epoch 0/29
----------
last_layer ---> train Loss: 1.0538 Acc: 0.7038
last_layer ---> val Loss: 0.5340 Acc: 0.8430

Epoch 1/29
----------
last_layer ---> train Loss: 0.8043 Acc: 0.7578
last_layer ---> val Loss: 0.4516 Acc: 0.8599

Epoch 2/29
----------
last_layer ---> train Loss: 0.7719 Acc: 0.7655
last_layer ---> val Loss: 0.4430 Acc: 0.8663

Epoch 3/29
----------
last_layer ---> train Loss: 0.7596 Acc: 0.7702
last_layer ---> val Loss: 0.4187 Acc: 0.8721

Epoch 4/29
----------
last_layer ---> train Loss: 0.7415 Acc: 0.7764
last_layer ---> val Loss: 0.4108 Acc: 0.8747

Epoch 5/29
----------
last_layer ---> train Loss: 0.7317 Acc: 0.7793
last_layer ---> val Loss: 0.3826 Acc: 0.8848

Epoch 6/29
----------
last_layer ---> train Loss: 0.7366 Acc: 0.7756
last_layer ---> val Loss: 0.4073 Acc: 0.8784

Epoch 7/29
----------
last_layer ---> train Loss: 0.7302 Acc: 0.7800
last_layer ---> val Loss: 0.5070 Acc: 0.8686

Epoch 8/29
----------
last_layer ---> train Loss: 0.7146 Acc: 0.7830
last_layer ---> val Loss: 0.3989 Acc: 0.8782

Epoch 9/29
----------
last_layer ---> train Loss: 0.7217 Acc: 0.7825
last_layer ---> val Loss: 0.4059 Acc: 0.8788

Epoch 10/29
----------
last_layer ---> train Loss: 0.7241 Acc: 0.7829
last_layer ---> val Loss: 0.3860 Acc: 0.8831

Epoch 11/29
----------
last_layer ---> train Loss: 0.7233 Acc: 0.7831
last_layer ---> val Loss: 0.3752 Acc: 0.8846

Epoch 12/29
----------
last_layer ---> train Loss: 0.7193 Acc: 0.7840
last_layer ---> val Loss: 0.4028 Acc: 0.8814

Epoch 13/29
----------
last_layer ---> train Loss: 0.7063 Acc: 0.7877
last_layer ---> val Loss: 0.3696 Acc: 0.8840

Epoch 14/29
----------
last_layer ---> train Loss: 0.7129 Acc: 0.7865
last_layer ---> val Loss: 0.3684 Acc: 0.8913

Epoch 15/29
----------
last_layer ---> train Loss: 0.7226 Acc: 0.7836
last_layer ---> val Loss: 0.3637 Acc: 0.8868

Epoch 16/29
----------
last_layer ---> train Loss: 0.7155 Acc: 0.7836
last_layer ---> val Loss: 0.3506 Acc: 0.8913

Epoch 17/29
----------
last_layer ---> train Loss: 0.7140 Acc: 0.7857
last_layer ---> val Loss: 0.3506 Acc: 0.8911

Epoch 18/29
----------
last_layer ---> train Loss: 0.7164 Acc: 0.7840
last_layer ---> val Loss: 0.3484 Acc: 0.8918

Epoch 19/29
----------
last_layer ---> train Loss: 0.7245 Acc: 0.7841
last_layer ---> val Loss: 0.3862 Acc: 0.8837

Epoch 20/29
----------
last_layer ---> train Loss: 0.7082 Acc: 0.7884
last_layer ---> val Loss: 0.3710 Acc: 0.8886

Epoch 21/29
----------
last_layer ---> train Loss: 0.7187 Acc: 0.7868
last_layer ---> val Loss: 0.3591 Acc: 0.8868

Epoch 22/29
----------
last_layer ---> train Loss: 0.7065 Acc: 0.7857
last_layer ---> val Loss: 0.3718 Acc: 0.8877

Epoch 23/29
----------
last_layer ---> train Loss: 0.7185 Acc: 0.7850
last_layer ---> val Loss: 0.3682 Acc: 0.8885

Epoch 24/29
----------
last_layer ---> train Loss: 0.7082 Acc: 0.7887
last_layer ---> val Loss: 0.3491 Acc: 0.8913

Epoch 25/29
----------
last_layer ---> train Loss: 0.7027 Acc: 0.7895
last_layer ---> val Loss: 0.3664 Acc: 0.8874

Epoch 26/29
----------
last_layer ---> train Loss: 0.7064 Acc: 0.7892
last_layer ---> val Loss: 0.3522 Acc: 0.8911

Epoch 27/29
----------
last_layer ---> train Loss: 0.7094 Acc: 0.7888
last_layer ---> val Loss: 0.3687 Acc: 0.8881

Epoch 28/29
----------
last_layer ---> train Loss: 0.7126 Acc: 0.7865
last_layer ---> val Loss: 0.3227 Acc: 0.9001

Epoch 29/29
----------
last_layer ---> train Loss: 0.7032 Acc: 0.7896
last_layer ---> val Loss: 0.4080 Acc: 0.8820

last_layer ---> Training complete in 178m 43s
last_layer ---> Best val Acc: 0.900090
############################################## last_layer ###############################################################
