PyTorch Version:  1.12.1
Torchvision Version:  0.13.1
The selected epochs is: 30
The selected feature_extract is: True
The selected use_pretrained is: True
The selected mode is: last_layer
The selected device is: cuda:1
VGG(
  (features): Sequential(
    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
    (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (5): ReLU(inplace=True)
    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (7): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (9): ReLU(inplace=True)
    (10): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (11): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (12): ReLU(inplace=True)
    (13): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (14): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (15): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (16): ReLU(inplace=True)
    (17): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (18): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (19): ReLU(inplace=True)
    (20): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (21): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (22): ReLU(inplace=True)
    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (24): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (25): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (26): ReLU(inplace=True)
    (27): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (28): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (29): ReLU(inplace=True)
    (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (31): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (32): ReLU(inplace=True)
    (33): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (35): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (36): ReLU(inplace=True)
    (37): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (38): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (39): ReLU(inplace=True)
    (40): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (41): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (42): ReLU(inplace=True)
    (43): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))
  (classifier): Sequential(
    (0): Linear(in_features=25088, out_features=4096, bias=True)
    (1): ReLU(inplace=True)
    (2): Dropout(p=0.5, inplace=False)
    (3): Linear(in_features=4096, out_features=4096, bias=True)
    (4): ReLU(inplace=True)
    (5): Dropout(p=0.5, inplace=False)
    (6): Linear(in_features=4096, out_features=39, bias=True)
  )
)
Params to learn:
	 classifier.6.weight
	 classifier.6.bias
Epoch 0/29
----------
last_layer ---> train Loss: 1.0559 Acc: 0.7050
last_layer ---> val Loss: 0.5216 Acc: 0.8433

Epoch 1/29
----------
last_layer ---> train Loss: 0.8045 Acc: 0.7602
last_layer ---> val Loss: 0.4768 Acc: 0.8560

Epoch 2/29
----------
last_layer ---> train Loss: 0.7631 Acc: 0.7692
last_layer ---> val Loss: 0.4506 Acc: 0.8605

Epoch 3/29
----------
last_layer ---> train Loss: 0.7573 Acc: 0.7709
last_layer ---> val Loss: 0.4167 Acc: 0.8723

Epoch 4/29
----------
last_layer ---> train Loss: 0.7476 Acc: 0.7724
last_layer ---> val Loss: 0.3964 Acc: 0.8779

Epoch 5/29
----------
last_layer ---> train Loss: 0.7385 Acc: 0.7760
last_layer ---> val Loss: 0.3979 Acc: 0.8744

Epoch 6/29
----------
last_layer ---> train Loss: 0.7324 Acc: 0.7795
last_layer ---> val Loss: 0.4219 Acc: 0.8735

Epoch 7/29
----------
last_layer ---> train Loss: 0.7287 Acc: 0.7802
last_layer ---> val Loss: 0.3825 Acc: 0.8819

Epoch 8/29
----------
last_layer ---> train Loss: 0.7228 Acc: 0.7816
last_layer ---> val Loss: 0.3725 Acc: 0.8829

Epoch 9/29
----------
last_layer ---> train Loss: 0.7235 Acc: 0.7830
last_layer ---> val Loss: 0.3883 Acc: 0.8800

Epoch 10/29
----------
last_layer ---> train Loss: 0.7256 Acc: 0.7810
last_layer ---> val Loss: 0.4019 Acc: 0.8765

Epoch 11/29
----------
last_layer ---> train Loss: 0.7267 Acc: 0.7825
last_layer ---> val Loss: 0.4327 Acc: 0.8752

Epoch 12/29
----------
last_layer ---> train Loss: 0.7247 Acc: 0.7826
last_layer ---> val Loss: 0.3711 Acc: 0.8883

Epoch 13/29
----------
last_layer ---> train Loss: 0.7160 Acc: 0.7853
last_layer ---> val Loss: 0.3710 Acc: 0.8844

Epoch 14/29
----------
last_layer ---> train Loss: 0.7181 Acc: 0.7850
last_layer ---> val Loss: 0.3741 Acc: 0.8788

Epoch 15/29
----------
last_layer ---> train Loss: 0.7130 Acc: 0.7851
last_layer ---> val Loss: 0.3887 Acc: 0.8815

Epoch 16/29
----------
last_layer ---> train Loss: 0.7120 Acc: 0.7859
last_layer ---> val Loss: 0.3398 Acc: 0.8931

Epoch 17/29
----------
last_layer ---> train Loss: 0.7160 Acc: 0.7850
last_layer ---> val Loss: 0.3682 Acc: 0.8911

Epoch 18/29
----------
last_layer ---> train Loss: 0.7168 Acc: 0.7862
last_layer ---> val Loss: 0.3703 Acc: 0.8835

Epoch 19/29
----------
last_layer ---> train Loss: 0.7105 Acc: 0.7890
last_layer ---> val Loss: 0.3954 Acc: 0.8821

Epoch 20/29
----------
last_layer ---> train Loss: 0.6983 Acc: 0.7908
last_layer ---> val Loss: 0.3729 Acc: 0.8842

Epoch 21/29
----------
last_layer ---> train Loss: 0.7224 Acc: 0.7836
last_layer ---> val Loss: 0.3604 Acc: 0.8887

Epoch 22/29
----------
last_layer ---> train Loss: 0.7069 Acc: 0.7874
last_layer ---> val Loss: 0.3636 Acc: 0.8913

Epoch 23/29
----------
last_layer ---> train Loss: 0.7119 Acc: 0.7890
last_layer ---> val Loss: 0.3672 Acc: 0.8858

Epoch 24/29
----------
last_layer ---> train Loss: 0.7050 Acc: 0.7903
last_layer ---> val Loss: 0.3570 Acc: 0.8892

Epoch 25/29
----------
last_layer ---> train Loss: 0.7164 Acc: 0.7873
last_layer ---> val Loss: 0.3985 Acc: 0.8817

Epoch 26/29
----------
last_layer ---> train Loss: 0.6940 Acc: 0.7922
last_layer ---> val Loss: 0.3504 Acc: 0.8891

Epoch 27/29
----------
last_layer ---> train Loss: 0.7043 Acc: 0.7886
last_layer ---> val Loss: 0.3796 Acc: 0.8867

Epoch 28/29
----------
last_layer ---> train Loss: 0.7162 Acc: 0.7859
last_layer ---> val Loss: 0.3675 Acc: 0.8906

Epoch 29/29
----------
last_layer ---> train Loss: 0.7131 Acc: 0.7863
last_layer ---> val Loss: 0.3490 Acc: 0.8904

last_layer ---> Training complete in 178m 48s
last_layer ---> Best val Acc: 0.893147
############################################## last_layer ###############################################################
