PyTorch Version:  1.12.1
Torchvision Version:  0.13.1
The selected epochs is: 30
The selected feature_extract is: True
The selected use_pretrained is: True
The selected mode is: last_layer
The selected device is: cuda:1
VGG(
  (features): Sequential(
    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
    (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (5): ReLU(inplace=True)
    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (7): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (9): ReLU(inplace=True)
    (10): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (11): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (12): ReLU(inplace=True)
    (13): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (14): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (15): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (16): ReLU(inplace=True)
    (17): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (18): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (19): ReLU(inplace=True)
    (20): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (21): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (22): ReLU(inplace=True)
    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (24): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (25): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (26): ReLU(inplace=True)
    (27): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (28): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (29): ReLU(inplace=True)
    (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (31): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (32): ReLU(inplace=True)
    (33): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (35): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (36): ReLU(inplace=True)
    (37): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (38): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (39): ReLU(inplace=True)
    (40): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (41): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (42): ReLU(inplace=True)
    (43): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))
  (classifier): Sequential(
    (0): Linear(in_features=25088, out_features=4096, bias=True)
    (1): ReLU(inplace=True)
    (2): Dropout(p=0.5, inplace=False)
    (3): Linear(in_features=4096, out_features=4096, bias=True)
    (4): ReLU(inplace=True)
    (5): Dropout(p=0.5, inplace=False)
    (6): Linear(in_features=4096, out_features=39, bias=True)
  )
)
Params to learn:
	 classifier.6.weight
	 classifier.6.bias
Epoch 0/29
----------
last_layer ---> train Loss: 0.8475 Acc: 0.5607
last_layer ---> val Loss: 0.1042 Acc: 0.1684

Epoch 1/29
----------
last_layer ---> train Loss: 0.6427 Acc: 0.6084
last_layer ---> val Loss: 0.0954 Acc: 0.1716

Epoch 2/29
----------
last_layer ---> train Loss: 0.6177 Acc: 0.6133
last_layer ---> val Loss: 0.0839 Acc: 0.1737

Epoch 3/29
----------
last_layer ---> train Loss: 0.6015 Acc: 0.6172
last_layer ---> val Loss: 0.0825 Acc: 0.1749

Epoch 4/29
----------
last_layer ---> train Loss: 0.5952 Acc: 0.6194
last_layer ---> val Loss: 0.0887 Acc: 0.1738

Epoch 5/29
----------
last_layer ---> train Loss: 0.5834 Acc: 0.6237
last_layer ---> val Loss: 0.0791 Acc: 0.1757

Epoch 6/29
----------
last_layer ---> train Loss: 0.5842 Acc: 0.6251
last_layer ---> val Loss: 0.0857 Acc: 0.1748

Epoch 7/29
----------
last_layer ---> train Loss: 0.5822 Acc: 0.6241
last_layer ---> val Loss: 0.0764 Acc: 0.1756

Epoch 8/29
----------
last_layer ---> train Loss: 0.5772 Acc: 0.6262
last_layer ---> val Loss: 0.0787 Acc: 0.1760

Epoch 9/29
----------
last_layer ---> train Loss: 0.5748 Acc: 0.6273
last_layer ---> val Loss: 0.0799 Acc: 0.1752

Epoch 10/29
----------
last_layer ---> train Loss: 0.5783 Acc: 0.6263
last_layer ---> val Loss: 0.0798 Acc: 0.1758

Epoch 11/29
----------
last_layer ---> train Loss: 0.5739 Acc: 0.6281
last_layer ---> val Loss: 0.0748 Acc: 0.1763

Epoch 12/29
----------
last_layer ---> train Loss: 0.5745 Acc: 0.6270
last_layer ---> val Loss: 0.0721 Acc: 0.1777

Epoch 13/29
----------
last_layer ---> train Loss: 0.5753 Acc: 0.6260
last_layer ---> val Loss: 0.0757 Acc: 0.1771

Epoch 14/29
----------
last_layer ---> train Loss: 0.5729 Acc: 0.6272
last_layer ---> val Loss: 0.0882 Acc: 0.1749

Epoch 15/29
----------
last_layer ---> train Loss: 0.5807 Acc: 0.6274
last_layer ---> val Loss: 0.0768 Acc: 0.1768

Epoch 16/29
----------
last_layer ---> train Loss: 0.5695 Acc: 0.6290
last_layer ---> val Loss: 0.0770 Acc: 0.1773

Epoch 17/29
----------
last_layer ---> train Loss: 0.5762 Acc: 0.6274
last_layer ---> val Loss: 0.0705 Acc: 0.1785

Epoch 18/29
----------
last_layer ---> train Loss: 0.5724 Acc: 0.6289
last_layer ---> val Loss: 0.0674 Acc: 0.1787

Epoch 19/29
----------
last_layer ---> train Loss: 0.5688 Acc: 0.6301
last_layer ---> val Loss: 0.0741 Acc: 0.1763

Epoch 20/29
----------
last_layer ---> train Loss: 0.5713 Acc: 0.6284
last_layer ---> val Loss: 0.0721 Acc: 0.1773

Epoch 21/29
----------
last_layer ---> train Loss: 0.5728 Acc: 0.6289
last_layer ---> val Loss: 0.0764 Acc: 0.1766

Epoch 22/29
----------
last_layer ---> train Loss: 0.5758 Acc: 0.6272
last_layer ---> val Loss: 0.0749 Acc: 0.1764

Epoch 23/29
----------
last_layer ---> train Loss: 0.5664 Acc: 0.6299
last_layer ---> val Loss: 0.0727 Acc: 0.1774

Epoch 24/29
----------
last_layer ---> train Loss: 0.5681 Acc: 0.6303
last_layer ---> val Loss: 0.0709 Acc: 0.1774

Epoch 25/29
----------
last_layer ---> train Loss: 0.5657 Acc: 0.6318
last_layer ---> val Loss: 0.0738 Acc: 0.1782

Epoch 26/29
----------
last_layer ---> train Loss: 0.5654 Acc: 0.6321
last_layer ---> val Loss: 0.0699 Acc: 0.1787

Epoch 27/29
----------
last_layer ---> train Loss: 0.5602 Acc: 0.6325
last_layer ---> val Loss: 0.0719 Acc: 0.1770

Epoch 28/29
----------
last_layer ---> train Loss: 0.5686 Acc: 0.6297
last_layer ---> val Loss: 0.0781 Acc: 0.1760

Epoch 29/29
----------
last_layer ---> train Loss: 0.5715 Acc: 0.6297
last_layer ---> val Loss: 0.0746 Acc: 0.1762

last_layer ---> Training complete in 181m 48s
last_layer ---> Best val Acc: 0.178744
############################################## last_layer ###############################################################
